#Лабораторная работа №3

**Тема**: Генерация текста в стиле творчества Валерия Меладзе  

**Цель**: Познакомиться с задачей генерации текста и применить предобученную модель GPT-3 Medium для генерации произведений в стиле выбранного исполнителя с использованием метода fine-tuning  

**Исполнители**:  
- Чупахина В.В.  
- Фомичева П.Ю.  
- Салимова А.Ф.
## Содержание

- [Теоретическая часть](#теоретическая-часть)
- [Описание разработанной системы](#описание-разработанной-системы-алгоритмы-принципы-работы-архитектура)
- [Результаты](#результаты)
- [Выводы](#выводы)
- [Список использованных источников](#список-использованных-источников)

## Теоретическая часть
### Задача генерации текста

Генерация текста — одна из ключевых задач обработки естественного языка (NLP), направленная на создание логически последовательного, грамматически правильного и семантически осмысленного текста. Она находит применение в различных областях: автоматический перевод, чат-боты, генерация резюме текстов, креативное письмо, программирование и др.
В задаче генерации текста модель получает на вход некоторый контекст (например, начало предложения, набор ключевых слов или вопрос) и должна выдать продолжение текста, которое максимально соответствует семантике и синтаксису входных данных.
Современные методы генерации текста используют архитектуры на основе трансформеров, такие как GPT, BART, T5 и другие. Эти модели обладают способностью захватывать длинные зависимости в тексте и учитывать широкий контекст, что существенно повышает качество генерируемого контента.

### Модель GPT-3 Medium: архитектура и особенности
<img width="887" alt="Снимок экрана 2025-05-04 в 23 42 03" src="https://github.com/user-attachments/assets/d1a815f8-3649-4ff8-96db-c82b97bac83e" />

*Рис. 1: Архитектура прогнозирования GPT-3 Medium*

GPT-3 Medium представляет собой среднюю по размеру версию в семействе моделей GPT-3 (Generative Pre-trained Transformer 3), разработанных OpenAI.

## Ключевые характеристики

| Характеристика          | Значение                  |
|-------------------------|--------------------------|
| Объем параметров        | 6.7 миллиардов           |
| Архитектура             | Декодер-трансформер      |
| Количество слоев        | 32                       |
| Размерность эмбеддингов | 4096                     |
| Механизм внимания       | 32-head self-attention   |
| Контекстное окно        | 2048 токенов             |

## Сравнение с другими версиями GPT-3
| Параметр           | GPT-3 Medium | GPT-3 Small | GPT-3 Large (175B) |
|--------------------|-------------|-------------|-------------------|
| Параметры          | 6.7B        | 125M        | 175B              |
| Слои               | 32          | 12          | 96                |
| Головы внимания    | 32          | 12          | 96                |
| Быстродействие     | ⚡⚡⚡        | ⚡⚡⚡⚡       | ⚡                 |

## Принципы генерации текста

GPT-3 Medium использует следующие механизмы:

1. **Авторегрессионная генерация**:
   - Последовательное предсказание токенов
   - Каждый новый токен генерируется на основе предыдущих

2. **Стратегии декодирования**:
   - **Temperature sampling**  
  T=0.7-1.0 для баланса креативности

  - **Top-p (nucleus) sampling**  
  p=0.9 для отсечения маловероятных вариантов

  - **Beam search**  
  При необходимости строгой согласованности

### Особенности fine-tuning

- **Адаптация только последних слоев**  
  PEFT-подход

- **Оптимальный learning rate**  
  1e-5 - 5e-5

- **Рекомендуемый размер батча**  
  4-8 (из-за ограничений VRAM)

### Метрики оценки качества генерации текста:
Оценка качества сгенерированных текстов является нетривиальной задачей, поскольку она должна учитывать как грамматическую правильность, так и смысловую релевантность. Существует ряд метрик, как автоматических, так и основанных на человеческой оценке:

- **chrF(++)** — символьная метрика, основанная на совпадении n-грамм, подходит для морфологически богатых языков.
- **Perplexity** — измеряет, насколько хорошо языковая модель предсказывает следующий токен. Чем ниже perplexity, тем лучше модель.
- **BLEU / ROUGE** — классические метрики на основе совпадения слов и фраз, применимы к задачам перевода и суммаризации.
- **Distinct-n**  — измеряет разнообразие генерируемых текстов, основываясь на количестве уникальных n-грамм.
- **Novelty** — показывает, насколько новый текст отличается от обучающего корпуса, важен для генерации креативных текстов.
- **LLM-as-judge** — сравнительно новый подход, при котором большая языковая модель (например, GPT-4) используется как оценщик качества других моделей на основе логических, семантических и стилистических критериев.

---
## Описание разработанной системы (алгоритмы, принципы работы, архитектура)
### Подход
Для генерации текстов в стиле Валерия Меладзе была использована модель **GPT-3 Medium**. Система обучалась на собранном датасете текстов песен, после чего применялась для генерации новых текстов.
### Алгоритм работы

#### 1. **Сбор и предобработка данных**:
   - Сбор текстов песен Валерия Меладзе
   - Очистка от технической информации (припев, куплет и т.д.)
   - Нормализация текста (приведение к нижнему регистру, удаление спецсимволов)

#### 2. Обучение модели GPT-3 Medium
Система использует предобученную модель GPT-3 Medium (на основе архитектуры GPT-2) со следующими параметрами обучения:

- **Количество эпох**: 6  
- **Скорость обучения**: 8e-5  
- **Длина последовательности**: 64 токена  
- **Размер тестовой выборки**: 10% данных
#### 3. Генерация текста

После обучения система предлагает два режима работы:
- **Интерактивная генерация**  
  Пользователь вводит начальную строку или название → модель генерирует продолжение

- **Пакетная оценка**  
  Автоматическая генерация текстов для тестовой выборки с расчетом метрик качества

**Параметры генерации**:
| Параметр | Значение |
|----------|----------|
| Максимальная длина | 40 токенов |
| Стратегия | Вероятностная выборка (top-p=0.92) |
| Температура | 1.1 |
| Штраф за повторения | 1.2 |

#### 4. Оценка качества

**Ключевые метрики**:
- `chrF++` - оценка качества генерации
- `Distinct-1/2` - разнообразие слов/биграмм
- `Novelty` - % новых слов
- `Perplexity` - уверенность модели

---
## Результаты
В результате выполнения работы была реализована модель GPT-3 Medium для генерации текстов в стиле Валерия Меладзе.
Основные результаты:
Модель успешно обучена на собранном датасете, демонстрируя способность генерировать тексты, стилистически близкие к оригинальным произведениям исполнителя.

Качество генерации подтверждено метриками chrF(++), Perplexity, Distinct-n и Novelty, а также экспертной оценкой через LLM-as-judge.
### Динамика метрик в процессе обучения

**Сравнительный анализ по эпохам:**

| Метрика       | 1 эпоха       | 5 эпох       | Δ Изменение | Интерпретация |
|---------------|---------------|--------------|------------|---------------|
| **chrF++**    | 17.19         | 49.65        | +188.9%    | Качество генерации значительно улучшилось |
| **Distinct-1**| 0.313         | 0.327        | +4.5%      | Незначительный рост уникальности слов |
| **Distinct-2**| 0.818         | 0.716        | -12.5%     | Снижение разнообразия биграмм |
| **Novelty %** | 52.60         | 27.51        | -47.7%     | Текст стал ближе к стилю оригинала |
| **Perplexity**| 21.43         | 6.65         | -68.9%     | Модель стала увереннее в предсказаниях |

### Визуализация прогресса

![5240255010934225033](https://github.com/user-attachments/assets/cc39a568-0fa2-46ed-8f11-8551e6824080)


Рисунок 1 - Примеры сгенерированных текстов

## Выводы

На основе проведенного эксперимента по тонкой настройке GPT-3 Medium можно сделать следующие выводы:

1. **Эффективность обучения**:
- Качество генерации значительно улучшилось (chrF++ вырос на 188.9%)
- Модель стала более уверенной в предсказаниях (перплексия снизилась на 68.9%)

2. **Особенности генерации**:
- Тексты стали ближе к стилю оригинала (снижение Novelty на 47.7%)
- Сохраняется высокая уникальность слов (рост Distinct-1 на 4.5%)
- Наблюдается снижение разнообразия сочетаний слов (Distinct-2 уменьшился на 12.5%)

Эксперимент подтвердил, что GPT-3 Medium хорошо адаптируется к стилевым особенностям творчества Валерия Меладзе.

## Список использованных источников

1. Raffel, C. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer / C. Raffel, N. Shazeer, A. Roberts [и др.] // arXiv preprint arXiv:1910.10683. – 2020. – URL: https://arxiv.org/abs/1910.10683 (дата обращения: 14.04.2025).
2. Hugging Face Transformers : документация. – 2024 – URL: https://huggingface.co/docs/transformers (дата обращения: 14.04.2025).
3. Google Research. Text-to-Text Transfer Transformer (T5) : репозиторий. – URL: https://github.com/google-research/text-to-text-transfer-transformer (дата обращения: 14.04.2025).
4. Васильев, А. М. Искусственный интеллект и обработка естественного языка : учебное пособие / А. М. Васильев. – Москва : КНОРУС, 2022. – 304 с. – ISBN 978-5-406-09581-2.
5. Бондарев, А. В. Глубокое обучение и трансформеры : от BERT до GPT / А. В. Бондарев. – Санкт-Петербург : Питер, 2023. – 288 с. – ISBN 978-5-4461-1873-1.
